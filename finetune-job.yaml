apiVersion: batch/v1
kind: Job
metadata:
  name: llama3-finetune-job
spec:
  template:
    spec:
      serviceAccountName: llama3-finetuner-ksa # IAM Service Accountì™€ ì—°ë™ëœ Kubernetes Service Account ì‚¬ìš©
      restartPolicy: Never # ë””ë²„ê¹…ì„ ìœ„í•´ ì‹¤íŒ¨ ì‹œ ë‹¤ì‹œ ì‹œì‘í•˜ì§€ ì•ŠìŒ
      nodeSelector:
        cloud.google.com/gke-accelerator: nvidia-h100-mega-80gb
      containers:
      - name: finetuner
        image: <YOUR_IMAGE_URI> # ğŸš¨ push.shë¡œ ë¹Œë“œí•œ ë³¸ì¸ì˜ ì´ë¯¸ì§€ URIë¡œ ë³€ê²½í•˜ì„¸ìš”.
        command:
        - "bash"
        - "-c"
        - "python /app/scripts/finetune.py"
        env:
        - name: GCS_BUCKET_NAME
          value: "<YOUR_GCS_BUCKET_NAME>" # ğŸš¨ ë³¸ì¸ì˜ GCS ë²„í‚· ì´ë¦„ìœ¼ë¡œ ë³€ê²½í•˜ì„¸ìš”.
        - name: HUGGING_FACE_HUB_TOKEN
          valueFrom:
            secretKeyRef:
              name: huggingface-secret
              key: token
        volumeMounts:
        - mountPath: /dev/shm
          name: dshm
        - mountPath: /tmp
          name: tmp
        resources:
          limits:
            nvidia.com/gpu: 8
        securityContext:
          privileged: true
      volumes:
      - name: dshm
        emptyDir:
          medium: Memory
          sizeLimit: 128Gi # A3 Mega ì¸ìŠ¤í„´ìŠ¤ì˜ ëŒ€ìš©ëŸ‰ ë©”ëª¨ë¦¬ë¥¼ í™œìš©
      - name: tmp
        emptyDir: {}
      hostIPC: true
      tolerations:
      - key: "nvidia.com/gpu"
        operator: "Exists"
        effect: "NoSchedule"
  backoffLimit: 1
